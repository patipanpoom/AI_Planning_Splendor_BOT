## Conclusion

From our experiments, including BFS, Greedy, MCTS, and Minimax, each had their strengths and weaknesses. BFS excelled in its ability to find the shortest path to victory but lacked long-term planning capabilities. Greedy, while straightforward, often made suboptimal decisions and struggled to adapt to opponent strategies. MCTS showed promise in its ability to make powerful moves but struggled with long-term planning and efficient resource utilization. Minimax performed well in predicting optimal opponent moves but lacked adaptability to dynamic game conditions. 

Q-learning, on the other hand, has the ability to prioritize gems that provide long-term rewards and adapt to changing game conditions. Despite the challenges posed by the game's complexity, Q-learning demonstrated its effectiveness in making good decisions. Moreover, our Q-learning was implemented to have a better strategic planning and adaptability. By continuously learning and updating its strategies through gameplay, Q-learning was able to make decisions for actions more effectively in the game with dynamic environment like Splendor.